\section{Site cost estimation}
The purpose of site cost estimation is to understand and measure what
the data centres typical expenses are, and predict what they may be in
the future. Our approach is to model those expenses taking into
account the diversity of national contexts across sites in terms of
funding, procurement procedures, and local market conditions.  These
results will help experiments plan new computing strategies that will
improve the cost-effectiveness of their resource usage.

\subsection{First results}
Some of the first elements to address are the diversity of expenses
across sites and their definition.  A simple and quick exercise was
made by four sites, aiming to get a first estimate of the financial
cost to run a given workflow and to store a given amount of data, in
terms of IT resources and power consumption.  The answers to this
exercise differed up to a factor of two from site to site.
The reasons were found to originate from the intrinsic variety of
costs, the measurement method, and the understanding of what a given
metric means.

Those results clearly showed the need of a consolidated and common
model and method to measure costs across WLCG data centres. For
example, one may consider the cost of a server providing a given
capacity, including (or not) the equipment that is shipped with it
(rack, switch, adaptor etc.).  One may also include (or not) in a unit
of capacity of tape storage the investments made in library, drives,
disk cache etc.\ that are needed for such system to work.  Finally,
the measurement of the electrical consumption may include (or not) the
Power Usage Effectiveness of the data centre in order to take into
account UPS or HVAC system contributions to the final power bill.  It
is therefore fundamental for site cost estimation to establish a
precise definition of the cost-related metrics in order to build a
reliable model.

\subsection{Next steps}
An attempt to address the total cost of ownership (TCO)
through cost modeling was shown in~\cite{costmodel}. This model
assumes that a data centre invests each year a constant budget in the
following assets: batch system, disk storage and tape system
capacities. If this hypothesis is satisfied (even roughly), one can
show that budget ($B$) and available capacity ($K$) over time are
bound together by a quantity ($c^*$) that depends on hardware cost
evolution and lifetime:

\begin{equation}
    B (t) = K (t) \times c^* (t)
    \label{eq:costmodel}
\end{equation}

In the case where hardware unitary costs decrease exponentially over
time with a rate $r$ (e.g. 0.2 for a 20\% yearly decrease) and
hardware is replaced after $\tau$ years, one can show that

\begin{equation}
c^*(t)=c(t)\frac{r}{1-(1-r)^\tau}
\end{equation}
and the site capacity for a flat budget turns out to be exponentially
increasing.

This model however does not address all the components of a TCO, like
manpower.  Site cost studies may leverage this model to estimate the
financial impact of a variation in the usage that experiments make of
data centre resources.  But the quantity $c^*$ may differ
significantly from site to site, so an extension of this model at
global scale will have to take into account as many as site-dependent
parameters as possible to establish $c^*$.

In order to get a better idea of the variations of the expenditure at
different sites, a survey is being conducted at all Tier-1 sites,
although the results are not yet available.
