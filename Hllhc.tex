\section{Areas of improvement}
Since the start of the LHC the community constantly improved the
throughput of the main workflows; however, studies conducted by the
Understanding Performance team and this working group found several
areas in which potential improvements could be
achieved~\cite{improvements}.

\subsection{Compiler and software improvements}
WLCG sites provide a variety of CPU models and HEP code is usually
built to run on the oldest available architectures. This, and
advancements in compilers motivated further studies.

Studies on GEANT simulation, reconstruction and NLO generator code
show gains by compiler and link-based optimisations to be around
20-25\%, including compilation for individual target CPUs, the use of
Intelâ€™s commercial compiler and the use of feedback-directed
optimisation (AutoFDO from Google and Intel). Compiler-based
vectorisation of current production code showed none or minimal
improvements. Reducing the overhead of shared libraries by building
large libraries resulted in large gains on older architectures (e.g.,
a 10\% on Ivy Bridge for ATLAS simulation). It has to be noted that
the use of feedback-directed optimisation requires that the objects
are build statically, which is not always trivial.

From code profiling and a detailed analysis of the dynamic use of
memory allocation, it is known that the current code spends up to 25\%
of the time on memory/object management, due to the frequent creation
and destruction of small objects~\cite{fomtools}. A 60-90\% of
allocations exists for less than $100\mu$s and are smaller than 64
bytes.  By using better strategies for object management, such as
object pools and static vectors, this could be reduced to less than
10\% with relatively minor code refactorisation. At the same time the
data structure layout can be improved for more efficient utilisation
of caches and improved memory access.

The current HEP code executes on modern cores only $0.8-1.5$
instructions per cycle. With the large vector registers provided by
current CPUs the theoretical limit is above 20 IPC for fully
vectorised code, and tuned complex code for HPC systems can reach
about 4 IPC, which can be seen as an upper limit for our code
base. Approaching this limit requires at least a change of the used
data structures and a re-implementation or factorisation of our
algorithms, and therefore it should be taken into consideration when
designing new algorithms.

Gains from new algorithms and the impact of new detector components
cannot be treated here. As the development of the cellular automaton
based, HLT track reconstruction code for the ALICE HLT has shown,
large factors O(100) can in some cases be achieved~\cite{rohr}.

\subsection{Storage}
The LHC community has recently agreed to a scenario where managed
storage is consolidated at a few, very large data centres (the ``data
lake'') as the most promising to achieve significant cost
savings~\cite{cwp}.

For what concerns operational effort, the 2015 WLCG site
survey~\cite{survey} showed that on average Tier-1 sites require 2.5
FTEs for operating storage and Tier-2 sites 0.75 FTEs, with a weak
dependence on the amount of storage. By concentrating managed storage
at a few sites and using only disk caches at
most sites, one can estimate a decrease of the overall number of FTE
for storage operations from around 100 to around 60.

An $80\%$ of used disk in WLCG consists of data formats used for
analysis. Data popularity studies show that on average datasets exist
at $\sim2$ sites and are accessed less than 10 times over a period of
six months, with most accesses in the first month.  We can argue that
data needs to be stored only once in the data lake, and temporarily
cached as needed depending on the client load. Less popular data may
be purged from disk and kept on tape.
Again, analysis of popularity data and storage system monitoring
indicates that only a small fraction of the produced data is active at
any time and a significant fraction (15-20\%) of the data could be
moved to a different storage layer.

Potential savings depend on the retention strategy and the use of
hierarchical storage: what is gained from replacing some amount of
disk with tape is partially offset by the need of more tape drives and
some added complexity in the data migration.

The concentration of data at a few sites requires to limit the impact
of bandwidth limitations and latency on the workload throughput.  We
measured the impact of latency on throughput of various workloads,
showing that for latencies up to 25ms the reduction of throughput can
be limited to 5\% when using Xcache as an additional layer
(table~\ref{tab:latency}).
\begin{table}
  \centering
  \caption{ Summary of cache and latency studies}
  \label{tab:latency}
  \begin{tabular}{llrlr}
    \hline
    Workload & Condition & Latency (ms) & Method & Rel. time \\\hline
    ATLAS Digi-Reco & data on node & 0 & running local & 1.00 \\ 
    ATLAS Digi-Reco & data remote & ~25 & running local & 1.90 \\ 
    ATLAS Digi-Reco & data rem., empty cache & ~25 & Xcache & 1.08 \\ 
    ATLAS Digi-Reco & data rem., pop. cache & ~25 & Xcache & 1.04 \\ 
    ATLAS Derivation & data on node & 0 & running local & 1.00 \\ 
    ATLAS Derivation & data on EOS & < 1 & running local  & 1.02 \\ 
    ATLAS Derivation & data remote & ~25 & running local & 8.30 \\ 
    ATLAS Derivation & data rem., empty cache & ~25 & Xcache & 1.05 \\ 
    ATLAS Derivation & data rem., pop. cache & ~25 & Xcache & 1.03 \\
    CMS Digi-Reco & data local & 0 & added latency & 1.00 \\
    CMS Digi-Reco & data local & 5 & added latency & 1.01 \\
    CMS Digi-Reco & data local & 10 & added latency & 1.04 \\
    CMS Digi-Reco & data local & 20 & added latency & 1.11 \\
    CMS Digi-Reco & data local & 50 & added latency & 1.24 \\\hline
  \end{tabular}
\end{table}

Data redundancy within storage systems is achieved either by
replication (more performant but expensive) or some form of error
encoding (cheaper but less performant). Even larger savings can be
achieved by not using data redundancy at all and re-staging from
tape, or possibly even regenerating, any lost data.
Based on the observed disk failure rate of about 1\% per year in the
CERN EOS system, the relative total cost
of storage and computing at CERN (around 4 HS06/TB), the amount of CPU
time to generate AOD events ($\sim 850$ HS06$\cdot s$) and their size
($\sim 400$ kB), one can naively estimate that the computing cost to
re-generate the AOD data lost to disk failures is $\sim 20\%$ of the
cost to make the storage fully redundant.

In reality, most of the times the lost data would be replicated
elsewhere; one can conservatively estimate that 30-50\% of the disc
costs can be saved. For this approach to be efficient, the process of
recreating individual files has to be automated, which is highly
desirable since other failure modes lead to data loss on a comparable
scale.

\subsection{Gains from improvements in operations}
Scheduling inefficiencies in WLCG can arise from a mismatch between
cores in a system and memory requirements, a mismatch between
requested cores and cores grouped on nodes (the tessellation problem),
batch system or pilot service inefficiencies, delays due to data
staging, I/O waits etc. Site managers have identified some of these
problems and $20-30\%$ of resources may be lost due to them. With
advanced backfilling and more complex job placement strategies,
efficiencies above 90\% can be reached, at the expense of added
complexity in workload management systems. Another source of
scheduling inefficiencies stems from breaking the processing chains
from raw data to data analysis objects into several individual steps
that exchange data via files: especially when parallel processing
threads/processes write individual files, the merging steps create
inefficiencies, as they are single threaded. By using shared writers
these inefficiencies can be reduced. The overall impact is difficult
to measure, but from ATLAS step chain activity logs it can be
estimated to be about 5\%.

Losses due to occasional job failures are unavoidable. Currently most
of the steps can recover from failure with the help of automated retry
mechanisms, but complete jobs are sometimes run up to 20 times before
successful completion. The overall loss in walltime due to job
failures is between 10 and 15\%. Improved procedures to make jobs more
resilient or fail very early can reduce these losses.

Table~\ref{tab:pgain} summarises the different identified potential gains.

\begin{table}
  \centering
  \caption{Estimated potential gains and associate efforts}
  \label{tab:pgain}
  \begin{tabular}{llll}
    \hline
    Change & Effort for sites & Effort for users & Potential gain \\\hline
    Managed storage only at few sites & some on large sites & little & $-40\%$ effort \\
    Reduced data redundancy & some on large sites & some & $-30$-$50\%$ cost \\
    Scheduling and site inefficiencies & some & some & $+10$-$20\%$ CPU  \\
    Reduced job failure rates & little & some - massive & $+5$-$10\%$ CPU \\
    Compiler and build improvements & none &  little - some & $+15$-$20\%$ CPU \\
    Improved memory usage & none & some &  $+10$-$15\%$ CPU \\ 
    Exploiting modern CPU arch. &  none & massive & $+100\%$ CPU \\\hline
  \end{tabular}
\end{table}
